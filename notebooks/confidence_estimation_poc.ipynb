{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9b6ee2f7-0185-441d-a3b1-6b04c608e2bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext sparkmagic.magics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ec7aa5b8-1918-41f1-a8d9-b95837b794f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from IPython import get_ipython\n",
    "username = os.environ['RENKU_USERNAME']\n",
    "server = \"http://iccluster029.iccluster.epfl.ch:8998\"\n",
    "\n",
    "# set the application name as \"<your_gaspar_id>-homework3\"\n",
    "get_ipython().run_cell_magic(\n",
    "    'spark',\n",
    "    line='config', \n",
    "    cell=\"\"\"{{ \"name\": \"{0}-homework3\", \"executorMemory\": \"4G\", \"executorCores\": 4, \"numExecutors\": 10, \"driverMemory\": \"4G\"}}\"\"\".format(username)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "69212fdd-e72b-4ca9-bd0f-b0b2c04387ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>5983</td><td>application_1652960972356_1310</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://iccluster029.iccluster.epfl.ch:8088/proxy/application_1652960972356_1310/\">Link</a></td><td><a target=\"_blank\" href=\"http://iccluster060.iccluster.epfl.ch:8042/node/containerlogs/container_e05_1652960972356_1310_01_000001/eric\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "get_ipython().run_line_magic(\n",
    "    \"spark\", \"add -s {0}-homework3 -l python -u {1} -k\".format(username, server)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "842972c7-486d-4bc6-a409-f313335b3db5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are using Spark 2.3.2.3.1.4.0-315"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "print('We are using Spark %s' % spark.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "943dfaa7-c85d-4e41-9b6c-b61ebd1a6c45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[0;31mDocstring:\u001b[0m\n",
       "::\n",
       "\n",
       "  %spark [-c CONTEXT] [-s SESSION] [-o OUTPUT] [-q [QUIET]]\n",
       "             [-m SAMPLEMETHOD] [-n MAXROWS] [-r SAMPLEFRACTION] [-u URL]\n",
       "             [-a USER] [-p PASSWORD] [-t AUTH] [-l LANGUAGE] [-k [SKIP]]\n",
       "             [-i ID] [-e COERCE]\n",
       "             [command ...]\n",
       "\n",
       "Magic to execute spark remotely.\n",
       "\n",
       "This magic allows you to create a Livy Scala or Python session against a Livy endpoint. Every session can\n",
       "be used to execute either Spark code or SparkSQL code by executing against the SQL context in the session.\n",
       "When the SQL context is used, the result will be a Pandas dataframe of a sample of the results.\n",
       "\n",
       "If invoked with no subcommand, the cell will be executed against the specified session.\n",
       "\n",
       "Subcommands\n",
       "-----------\n",
       "info\n",
       "    Display the available Livy sessions and other configurations for sessions.\n",
       "add\n",
       "    Add a Livy session given a session name (-s), language (-l), and endpoint credentials.\n",
       "    The -k argument, if present, will skip adding this session if it already exists.\n",
       "    e.g. `%spark add -s test -l python -u https://sparkcluster.net/livy -t Kerberos -a u -p -k`\n",
       "config\n",
       "    Override the livy session properties sent to Livy on session creation. All session creations will\n",
       "    contain these config settings from then on.\n",
       "    Expected value is a JSON key-value string to be sent as part of the Request Body for the POST /sessions\n",
       "    endpoint in Livy.\n",
       "    e.g. `%%spark config`\n",
       "         `{\"driverMemory\":\"1000M\", \"executorCores\":4}`\n",
       "run\n",
       "    Run Spark code against a session.\n",
       "    e.g. `%%spark -s testsession` will execute the cell code against the testsession previously created\n",
       "    e.g. `%%spark -s testsession -c sql` will execute the SQL code against the testsession previously created\n",
       "    e.g. `%%spark -s testsession -c sql -o my_var` will execute the SQL code against the testsession\n",
       "             previously created and store the pandas dataframe created in the my_var variable in the\n",
       "             Python environment.\n",
       "logs\n",
       "    Returns the logs for a given session.\n",
       "    e.g. `%spark logs -s testsession` will return the logs for the testsession previously created\n",
       "delete\n",
       "    Delete a Livy session.\n",
       "    e.g. `%spark delete -s defaultlivy`\n",
       "cleanup\n",
       "    Delete all Livy sessions created by the notebook. No arguments required.\n",
       "    e.g. `%spark cleanup`\n",
       "\n",
       "positional arguments:\n",
       "  command               Commands to execute.\n",
       "\n",
       "optional arguments:\n",
       "  -c CONTEXT, --context CONTEXT\n",
       "                        Context to use: 'spark' for spark and 'sql' for sql\n",
       "                        queries. Default is 'spark'.\n",
       "  -s SESSION, --session SESSION\n",
       "                        The name of the Livy session to use.\n",
       "  -o OUTPUT, --output OUTPUT\n",
       "                        If present, output when using SQL queries will be\n",
       "                        stored in this variable.\n",
       "  -q <[QUIET]>, --quiet <[QUIET]>\n",
       "                        Do not display visualizations on SQL queries\n",
       "  -m SAMPLEMETHOD, --samplemethod SAMPLEMETHOD\n",
       "                        Sample method for SQL queries: either take or sample\n",
       "  -n MAXROWS, --maxrows MAXROWS\n",
       "                        Maximum number of rows that will be pulled back from\n",
       "                        the server for SQL queries\n",
       "  -r SAMPLEFRACTION, --samplefraction SAMPLEFRACTION\n",
       "                        Sample fraction for sampling from SQL queries\n",
       "  -u URL, --url URL     URL for Livy endpoint\n",
       "  -a USER, --user USER  Username for HTTP access to Livy endpoint\n",
       "  -p PASSWORD, --password PASSWORD\n",
       "                        Password for HTTP access to Livy endpoint\n",
       "  -t AUTH, --auth AUTH  Auth type for HTTP access to Livy endpoint. [Kerberos,\n",
       "                        None, Basic]\n",
       "  -l LANGUAGE, --language LANGUAGE\n",
       "                        Language for Livy session; one of python, scala, r\n",
       "  -k <[SKIP]>, --skip <[SKIP]>\n",
       "                        Skip adding session if it already exists\n",
       "  -i ID, --id ID        Session ID\n",
       "  -e COERCE, --coerce COERCE\n",
       "                        Whether to automatically coerce the types (default,\n",
       "                        pass True if being explicit) of the dataframe or not\n",
       "                        (pass False)\n",
       "\u001b[0;31mFile:\u001b[0m      /opt/conda/lib/python3.9/site-packages/sparkmagic/livyclientlib/exceptions.py\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da33cf-7422-4365-964e-840ff8f0dea7",
   "metadata": {},
   "source": [
    "### Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dfcfb580-a2e1-4dec-9540-43be4a603d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- stop_id: string (nullable = true)\n",
      " |-- stop_name: string (nullable = true)\n",
      " |-- stop_lat: double (nullable = true)\n",
      " |-- stop_lon: double (nullable = true)\n",
      " |-- location_type: string (nullable = true)\n",
      " |-- parent_station: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "all_stops = spark.read.orc('/data/sbb/orc/allstops')\n",
    "all_stops.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "221f28c1-6752-4bc3-a372-0172f0e25e85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- betriebstag: string (nullable = true)\n",
      " |-- fahrt_bezeichner: string (nullable = true)\n",
      " |-- betreiber_id: string (nullable = true)\n",
      " |-- betreiber_abk: string (nullable = true)\n",
      " |-- betreiber_name: string (nullable = true)\n",
      " |-- produkt_id: string (nullable = true)\n",
      " |-- linien_id: string (nullable = true)\n",
      " |-- linien_text: string (nullable = true)\n",
      " |-- umlauf_id: string (nullable = true)\n",
      " |-- verkehrsmittel_text: string (nullable = true)\n",
      " |-- zusatzfahrt_tf: string (nullable = true)\n",
      " |-- faellt_aus_tf: string (nullable = true)\n",
      " |-- bpuic: string (nullable = true)\n",
      " |-- haltestellen_name: string (nullable = true)\n",
      " |-- ankunftszeit: string (nullable = true)\n",
      " |-- an_prognose: string (nullable = true)\n",
      " |-- an_prognose_status: string (nullable = true)\n",
      " |-- abfahrtszeit: string (nullable = true)\n",
      " |-- ab_prognose: string (nullable = true)\n",
      " |-- ab_prognose_status: string (nullable = true)\n",
      " |-- durchfahrt_tf: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "actual_data = spark.read.orc('/data/sbb/orc/istdaten')\n",
    "actual_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1afb4057-fc3f-4ebe-a071-8e2443b7296e",
   "metadata": {},
   "source": [
    "### Sub-sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd3e26b1-c562-4feb-a756-043f34d92806",
   "metadata": {},
   "source": [
    "Sub-sample data for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "855e6f0f-6694-49c3-a2cf-5a5a1a065fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "# actual_data = actual_data.sample(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddef392d-3c73-4f7d-99bb-4a5692d88fd5",
   "metadata": {},
   "source": [
    "### Rename columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b84c582b-dc95-42ad-a3f4-c5e29cc6c90c",
   "metadata": {},
   "source": [
    "Columns are renamed in English as follows:\n",
    "- `betriebstag` to DATE\n",
    "- `produkt_id` to PRODUCT_ID\n",
    "- `fahrt_bezeichner` to TRIP_ID\n",
    "- `bpuic` to STOP_ID\n",
    "- `haltestellen_name` to STOP_NAME\n",
    "- `ankunftszeit` to ARRIVAL_TIME\n",
    "- `an_prognose_status` to ARRIVAL_TIME_STATUS\n",
    "- `an_prognose` to ACTUAL_ARRIVAL_TIME\n",
    "- `abfahrtszeit` to DEPARTURE_TIME\n",
    "- `ab_prognose_status` to DEPARTURE_TIME_STATUS\n",
    "- `ab_prognose` to ACTUAL_DEPARTURE_TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab4f83e8-2b79-4ae4-8032-2b2bc4e707a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#rename columns in english\n",
    "\n",
    "actual_data = actual_data.select(actual_data.betriebstag.alias('DATE'),\\\n",
    "                                 actual_data.produkt_id.alias('PRODUCT_ID'),\\\n",
    "                                 actual_data.bpuic.alias('STOP_ID'),\\\n",
    "                                 actual_data.haltestellen_name.alias('STOP_NAME'),\\\n",
    "                                 actual_data.fahrt_bezeichner.alias('TRIP_ID'),\\\n",
    "                                 actual_data.ankunftszeit.alias('ARRIVAL_TIME'),\\\n",
    "                                 actual_data.an_prognose_status.alias('ARRIVAL_TIME_STATUS'),\\\n",
    "                                 actual_data.an_prognose.alias('ACTUAL_ARRIVAL_TIME'),\\\n",
    "                                 actual_data.abfahrtszeit.alias('DEPARTURE_TIME'),\\\n",
    "                                 actual_data.ab_prognose_status.alias('DEPARTURE_TIME_STATUS'),\n",
    "                                 actual_data.ab_prognose.alias('ACTUAL_DEPARTURE_TIME'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11b6c661-672a-4b7f-83c2-684465dce04d",
   "metadata": {},
   "source": [
    "### Prepare table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b6285db9-0a91-4d30-a2de-d43501f753ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as F\n",
    "# F.date_format(F.to_timestamp(df_used.timestamp_s), 'yyyy-MM-dd HH:mm:ss').alias('date'))\n",
    "df = actual_data.withColumn('DATE', F.date_format(F.to_timestamp(actual_data.DATE, 'dd.MM.yyyy'), 'yyyy-MM-dd'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e3ff5ac-111f-41a1-8938-13df5094226d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "df = df.withColumn('DAY_OF_WEEK', ((F.dayofweek(df['DATE'])+5)%7)+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "970d287a-ec83-41cb-bf47-d1ba6e78499d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "#remove entries with ARRIVAL_TIME_STATUS and DEPARTURE_TIME_STATUS unknown\n",
    "df = df.filter(df['ARRIVAL_TIME_STATUS'] != 'UNBEKANNT')\n",
    "df = df.filter(df['DEPARTURE_TIME_STATUS'] != 'UNBEKANNT')\n",
    "\n",
    "#remove entries with ARRIVAL_TIME_STATUS and DEPARTURE_TIME_STATUS is empty\n",
    "df = df.filter(df['ARRIVAL_TIME_STATUS'] != '')\n",
    "df = df.filter(df['DEPARTURE_TIME_STATUS'] != '')\n",
    "\n",
    "#remove entries with PRODUCT_ID empty\n",
    "df = df.filter(df['PRODUCT_ID'] != '')\n",
    "\n",
    "#remove entries with STOP_ID empty\n",
    "df = df.filter(df['STOP_ID'] != '')\n",
    "\n",
    "#remove entries with STOP_NAME empty\n",
    "df = df.filter(df['STOP_NAME'] != '')\n",
    "\n",
    "#remove entries with TRIP_ID empty\n",
    "df = df.filter(df['TRIP_ID'] != '')\n",
    "\n",
    "#transform to unix_timestamp\n",
    "df = df.withColumn('ARRIVAL_TIME', F.unix_timestamp(df.ARRIVAL_TIME, 'dd.MM.yyyy HH:mm'))\n",
    "df = df.withColumn('ACTUAL_ARRIVAL_TIME', F.unix_timestamp(df.ACTUAL_ARRIVAL_TIME, 'dd.MM.yyyy HH:mm'))\n",
    "df = df.withColumn('DEPARTURE_TIME', F.unix_timestamp(df.DEPARTURE_TIME, 'dd.MM.yyyy HH:mm'))\n",
    "df = df.withColumn('ACTUAL_DEPARTURE_TIME', F.unix_timestamp(df.ACTUAL_DEPARTURE_TIME, 'dd.MM.yyyy HH:mm'))\n",
    "\n",
    "#fill null with 0 \n",
    "#When the stop is the start or end of a journey, the corresponding columns will be empty (ANKUNFTSZEIT/ABFAHRTSZEIT)\n",
    "df = df.na.fill({'ARRIVAL_TIME' : 0, 'ACTUAL_ARRIVAL_TIME' : 0, 'DEPARTURE_TIME' : 0, 'ACTUAL_DEPARTURE_TIME' : 0})\n",
    "\n",
    "#compute delay\n",
    "df = df.withColumn('ARRIVAL_DELAY', F.when((df['ACTUAL_ARRIVAL_TIME']-df['ARRIVAL_TIME'] )< 0, 0).otherwise(df['ACTUAL_ARRIVAL_TIME']-df['ARRIVAL_TIME']))\n",
    "df = df.withColumn('DEPARTURE_DELAY', F.when((df['ACTUAL_DEPARTURE_TIME']-df['DEPARTURE_TIME'] )< 0, 0).otherwise(df['ACTUAL_DEPARTURE_TIME']-df['DEPARTURE_TIME']))\n",
    "\n",
    "#convert seconds in minutes\n",
    "df = df.withColumn('ARRIVAL_DELAY', F.round(df['ARRIVAL_DELAY']/60))\n",
    "df = df.withColumn('DEPARTURE_DELAY', F.round(df['DEPARTURE_DELAY']/60))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7af87d01-bbc3-4e6e-a5de-f81885930322",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#define HOUR_ARRIVAL\n",
    "df = df.withColumn('HOUR_ARRIVAL', F.date_format(F.from_unixtime(df['ARRIVAL_TIME']), 'HH:mm:ss'))\n",
    "df = df.withColumn('HOUR_DEPARTURE', F.date_format(F.from_unixtime(df['DEPARTURE_TIME']), 'HH:mm:ss'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c16ecb6b-0632-4f9d-b597-210d10adbb50",
   "metadata": {},
   "source": [
    "### Filter data based on simplifying assumptions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5005cf36-db05-428f-a8c4-2d0600173c54",
   "metadata": {},
   "source": [
    "We only consider journeys at reasonable hours of the day, and on a typical business day.\n",
    "- `day`: from Monday to Friday\n",
    "- `hours`: first departure at 6:00, last arrival at 21:00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "edf90d89-2673-4528-86be-9840c32ac33c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#business day\n",
    "df = df.filter(df['DAY_OF_WEEK'] <6)\n",
    "#reasonable hours of the day\n",
    "df = df.filter(F.hour(F.from_unixtime(df['DEPARTURE_TIME']))>=8)\n",
    "df = df.filter(F.hour(F.from_unixtime(df['ARRIVAL_TIME']))<=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "db3668e6-3b04-419d-84ac-fbab5946b717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+--------------+-------+------------+\n",
      "|TRIP_ID            |HOUR_DEPARTURE|STOP_ID|STOP_NAME   |\n",
      "+-------------------+--------------+-------+------------+\n",
      "|80:06____:17010:000|08:28:00      |8500090|Basel Bad Bf|\n",
      "+-------------------+--------------+-------+------------+"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "df.filter(df.TRIP_ID == '80:06____:17010:000').orderBy(df.TRIP_ID, df.HOUR_DEPARTURE).select(df.TRIP_ID, df.HOUR_DEPARTURE, df.STOP_ID, df.STOP_NAME).distinct().show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eae628-79da-4709-aac0-522724b9bf57",
   "metadata": {},
   "source": [
    "### Obtain delay cumulative distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1fe51959-fa89-41aa-b37f-27c005ca5ec7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%spark\n",
    "from pyspark.sql import Window\n",
    "\n",
    "#compute cumulative distribution of delay over groupBy 'DAY_OF_WEEK', 'PRODUCT_ID', 'HOUR_ARRIVAL' \n",
    "cumulative_function_window = Window.partitionBy('DAY_OF_WEEK', 'TRIP_ID', 'HOUR_ARRIVAL', 'STOP_ID', 'STOP_NAME').orderBy('ARRIVAL_DELAY').rangeBetween(Window.unboundedPreceding, 0)\n",
    "count_window = Window.partitionBy('DAY_OF_WEEK', 'TRIP_ID', 'HOUR_ARRIVAL', 'STOP_ID', 'STOP_NAME')\n",
    "\n",
    "df2 = df.groupBy(df['DAY_OF_WEEK'], df['TRIP_ID'], df['HOUR_ARRIVAL'], df['STOP_ID'], df['STOP_NAME'], df['ARRIVAL_DELAY']).count()\n",
    "df2 = df2.withColumn(\"TOTAL_SUM\", F.sum(\"count\").over(count_window))\n",
    "df2 = df2.withColumn(\"PARTIAL_SUM\", F.sum(\"count\").over(cumulative_function_window))\n",
    "df2 = df2.withColumn(\"CUMULATIVE\", F.sum(\"count\").over(cumulative_function_window)/F.sum(\"count\").over(count_window))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bdf0f180-8801-43bf-a503-5c1b222f0475",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---------------+------------+-------+------------+-------------+-----+---------+-----------+-------------------+\n",
      "|DAY_OF_WEEK|        TRIP_ID|HOUR_ARRIVAL|STOP_ID|   STOP_NAME|ARRIVAL_DELAY|count|TOTAL_SUM|PARTIAL_SUM|         CUMULATIVE|\n",
      "+-----------+---------------+------------+-------+------------+-------------+-----+---------+-----------+-------------------+\n",
      "|          1|85:11:10157:001|    14:40:00|8501609|        Brig|          0.0|    5|        6|          5| 0.8333333333333334|\n",
      "|          1| 85:11:1067:001|    12:40:00|8501609|        Brig|          0.0|   34|       77|         34|0.44155844155844154|\n",
      "|          1| 85:11:1067:001|    12:40:00|8501609|        Brig|          1.0|   23|       77|         57| 0.7402597402597403|\n",
      "|          1| 85:11:1067:001|    12:40:00|8501609|        Brig|          2.0|    9|       77|         66| 0.8571428571428571|\n",
      "|          1| 85:11:1067:001|    12:40:00|8501609|        Brig|          3.0|    9|       77|         75|  0.974025974025974|\n",
      "|          1| 85:11:1067:001|    12:40:00|8501609|        Brig|          4.0|    1|       77|         76|  0.987012987012987|\n",
      "|          1|  85:11:106:002|    13:19:00|8500090|Basel Bad Bf|          1.0|    1|        2|          1|                0.5|\n",
      "|          1| 85:11:1078:001|    18:03:00|8500218|       Olten|          0.0|   41|      202|         41|0.20297029702970298|\n",
      "|          1| 85:11:1078:001|    18:03:00|8500218|       Olten|          1.0|  104|      202|        145| 0.7178217821782178|\n",
      "|          1| 85:11:1078:001|    18:03:00|8500218|       Olten|          2.0|   27|      202|        172| 0.8514851485148515|\n",
      "+-----------+---------------+------------+-------+------------+-------------+-----+---------+-----------+-------------------+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#print for understanding\n",
    "df2.filter(df2.CUMULATIVE < 1.0).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "132c6703-f01e-490c-a0e3-f7258f0f3ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "u'Cannot resolve column name \"PRODUCT_ID\" among (DAY_OF_WEEK, TRIP_ID, HOUR_ARRIVAL, STOP_ID, STOP_NAME, ARRIVAL_DELAY, count, TOTAL_SUM, PARTIAL_SUM, CUMULATIVE);'\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/eric/appcache/application_1652960972356_1310/container_e05_1652960972356_1310_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1161, in __getitem__\n",
      "    jc = self._jdf.apply(item)\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/eric/appcache/application_1652960972356_1310/container_e05_1652960972356_1310_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/eric/appcache/application_1652960972356_1310/container_e05_1652960972356_1310_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "AnalysisException: u'Cannot resolve column name \"PRODUCT_ID\" among (DAY_OF_WEEK, TRIP_ID, HOUR_ARRIVAL, STOP_ID, STOP_NAME, ARRIVAL_DELAY, count, TOTAL_SUM, PARTIAL_SUM, CUMULATIVE);'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "#reorder columns\n",
    "df2 = df2.select(df2[\"TRIP_ID\"], df2[\"STOP_ID\"], df2[\"DAY_OF_WEEK\"], df2[\"HOUR_ARRIVAL\"], df2[\"ARRIVAL_DELAY\"].alias(\"MAX_ARRIVAL_DELAY\"), df2[\"CUMULATIVE\"])\n",
    "df2 = df2.orderBy(df2[\"TRIP_ID\"], df2[\"HOUR_ARRIVAL\"], df2[\"HOUR_ARRIVAL\"], df2[\"MAX_ARRIVAL_DELAY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ba7ea5-15ab-4d95-83ab-3066e81d07e4",
   "metadata": {},
   "source": [
    "### Compute condifence given delay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "043663e5-9cf6-4ac2-9884-750f7732bda8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An error was encountered:\n",
      "u'Cannot resolve column name \"MAX_ARRIVAL_DELAY\" among (DAY_OF_WEEK, TRIP_ID, HOUR_ARRIVAL, STOP_ID, STOP_NAME, ARRIVAL_DELAY, count, TOTAL_SUM, PARTIAL_SUM, CUMULATIVE);'\n",
      "Traceback (most recent call last):\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/eric/appcache/application_1652960972356_1310/container_e05_1652960972356_1310_01_000001/pyspark.zip/pyspark/sql/dataframe.py\", line 1161, in __getitem__\n",
      "    jc = self._jdf.apply(item)\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/eric/appcache/application_1652960972356_1310/container_e05_1652960972356_1310_01_000001/py4j-0.10.7-src.zip/py4j/java_gateway.py\", line 1257, in __call__\n",
      "    answer, self.gateway_client, self.target_id, self.name)\n",
      "  File \"/hdata/sdf/hadoop/yarn/local/usercache/eric/appcache/application_1652960972356_1310/container_e05_1652960972356_1310_01_000001/pyspark.zip/pyspark/sql/utils.py\", line 69, in deco\n",
      "    raise AnalysisException(s.split(': ', 1)[1], stackTrace)\n",
      "AnalysisException: u'Cannot resolve column name \"MAX_ARRIVAL_DELAY\" among (DAY_OF_WEEK, TRIP_ID, HOUR_ARRIVAL, STOP_ID, STOP_NAME, ARRIVAL_DELAY, count, TOTAL_SUM, PARTIAL_SUM, CUMULATIVE);'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%%spark\n",
    "\n",
    "delay = 1 #minutes\n",
    "\n",
    "df2.filter(df2['MAX_ARRIVAL_DELAY'] == delay).show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73ce0686-af28-427d-ad07-827fd207055e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
