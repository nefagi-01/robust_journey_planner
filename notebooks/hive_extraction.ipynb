{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3969a56d-155d-43d8-947c-2ab9dad48e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operating as: eric\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "pd.set_option(\"display.max_columns\", 50)\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "#username = os.environ['RENKU_USERNAME']\n",
    "username = \"eric\"\n",
    "hiveaddr = os.environ['HIVE_SERVER2']\n",
    "(hivehost,hiveport) = hiveaddr.split(':')\n",
    "print(\"Operating as: {0}\".format(username))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d29dfeb9-806a-4007-a021-58befc36023e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyhive import hive\n",
    "\n",
    "# Create the connection\n",
    "conn = hive.connect(host=hivehost, \n",
    "                    port=hiveport,\n",
    "                    username=username) \n",
    "# Create the cursor\n",
    "cur = conn.cursor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d833f6d0-09d9-4784-9567-490134fc2d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Create your database if it does not exist\n",
    "query = \"\"\"\n",
    "CREATE DATABASE IF NOT EXISTS {0} LOCATION '/group/five-guys/hive'\n",
    "\"\"\".format(username)\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293f385d-9748-457a-acd7-dfc0f2110055",
   "metadata": {},
   "source": [
    "## Connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "902d6939-3e43-4d5f-b91b-e524e59f88ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_df_connections(table_name):\n",
    "    ### Create your database if it does not exist\n",
    "    \n",
    "    query = \"\"\"\n",
    "    DROP TABLE IF EXISTS {0}\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    ### Creation of the table\n",
    "    query = \"\"\"\n",
    "    CREATE EXTERNAL TABLE {0}(\n",
    "        departure_id string,\n",
    "        arrival_id string,\n",
    "        departure_time string,\n",
    "        arrival_time string,\n",
    "        trip_id string,\n",
    "        monday string,\n",
    "        tuesday string,\n",
    "        wednesday string,\n",
    "        thursday string,\n",
    "        friday string\n",
    "    )\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ','\n",
    "    STORED AS TEXTFILE\n",
    "    location '/group/five-guys/{0}'\n",
    "    tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM {0}.csv\n",
    "    \"\"\".format(table_name)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [column.split('.')[1] for column in df.columns]\n",
    "    \n",
    "    df[\"departure_time\"] = df[\"departure_time\"].apply(lambda x: int(pd.Timestamp(x).timestamp()))\n",
    "\n",
    "    df[\"arrival_time\"] = df[\"arrival_time\"].apply(lambda x: int(pd.Timestamp(x).timestamp()))\n",
    "    \n",
    "    df[['monday', 'tuesday', 'wednesday', 'thursday', 'friday']] = df[['monday', 'tuesday', 'wednesday', 'thursday', 'friday']].replace({'0':False, '1':True})\n",
    "\n",
    "    result = list(df.itertuples(index=False, name=None))\n",
    "    \n",
    "    del df\n",
    "    \n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1abb449b-5f45-4c81-99c9-c112196ac38b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.security.AccessControlException: Permission denied: user=ceraolo, access=WRITE, inode=\"/group/five-guys/conn_table\":eric:hadoop:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:261)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1841)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1791)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7804)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\\n\\tat java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\\n):28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:228', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:265', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:575', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:550', 'sun.reflect.GeneratedMethodAccessor46:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1730', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy42:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:285', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:567', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1142', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:617', 'java.lang.Thread:run:Thread.java:745', '*org.apache.hadoop.hive.ql.metadata.HiveException:MetaException(message:java.security.AccessControlException: Permission denied: user=ceraolo, access=WRITE, inode=\"/group/five-guys/conn_table\":eric:hadoop:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:261)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1841)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1791)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7804)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\\n\\tat java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\\n):39:12', 'org.apache.hadoop.hive.ql.metadata.Hive:createTable:Hive.java:1103', 'org.apache.hadoop.hive.ql.metadata.Hive:createTable:Hive.java:1108', 'org.apache.hadoop.hive.ql.exec.DDLTask:createTable:DDLTask.java:4801', 'org.apache.hadoop.hive.ql.exec.DDLTask:execute:DDLTask.java:400', 'org.apache.hadoop.hive.ql.exec.Task:executeTask:Task.java:212', 'org.apache.hadoop.hive.ql.exec.TaskRunner:runSequential:TaskRunner.java:103', 'org.apache.hadoop.hive.ql.Driver:launchTask:Driver.java:2712', 'org.apache.hadoop.hive.ql.Driver:execute:Driver.java:2383', 'org.apache.hadoop.hive.ql.Driver:runInternal:Driver.java:2055', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1753', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1747', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:run:ReExecDriver.java:157', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:226', '*org.apache.hadoop.hive.metastore.api.MetaException:java.security.AccessControlException: Permission denied: user=ceraolo, access=WRITE, inode=\"/group/five-guys/conn_table\":eric:hadoop:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:261)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1841)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1791)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7804)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\\n\\tat java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\\n:59:20', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme:read:ThriftHiveMetastore.java:56296', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme:read:ThriftHiveMetastore.java:56264', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result:read:ThriftHiveMetastore.java:56190', 'org.apache.thrift.TServiceClient:receiveBase:TServiceClient.java:86', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:recv_create_table_with_environment_context:ThriftHiveMetastore.java:1592', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:create_table_with_environment_context:ThriftHiveMetastore.java:1578', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:create_table_with_environment_context:HiveMetaStoreClient.java:3095', 'org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient:create_table_with_environment_context:SessionHiveMetaStoreClient.java:123', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createTable:HiveMetaStoreClient.java:915', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createTable:HiveMetaStoreClient.java:900', 'sun.reflect.GeneratedMethodAccessor106:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingMetaStoreClient:invoke:RetryingMetaStoreClient.java:212', 'com.sun.proxy.$Proxy36:createTable::-1', 'sun.reflect.GeneratedMethodAccessor106:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler:invoke:HiveMetaStoreClient.java:3001', 'com.sun.proxy.$Proxy36:createTable::-1', 'org.apache.hadoop.hive.ql.metadata.Hive:createTable:Hive.java:1092'], sqlState='08S01', errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.security.AccessControlException: Permission denied: user=ceraolo, access=WRITE, inode=\"/group/five-guys/conn_table\":eric:hadoop:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:261)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1841)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1791)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7804)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\\n\\tat java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\\n)'), operationHandle=None)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-1db03d98758d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconn_table\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mretrieve_df_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"conn_table\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-e3344dca1cfa>\u001b[0m in \u001b[0;36mretrieve_df_connections\u001b[0;34m(table_name)\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mtblproperties\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"skip.header.line.count\"\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"1\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \"\"\".format(table_name)\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mcur\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquery\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     query = \"\"\"\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36mexecute\u001b[0;34m(self, operation, parameters, **kwargs)\u001b[0m\n\u001b[1;32m    453\u001b[0m         \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    454\u001b[0m         \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExecuteStatement\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 455\u001b[0;31m         \u001b[0m_check_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    456\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_operationHandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moperationHandle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.9/site-packages/pyhive/hive.py\u001b[0m in \u001b[0;36m_check_status\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    583\u001b[0m     \u001b[0m_logger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatusCode\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mttypes\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTStatusCode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSUCCESS_STATUS\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 585\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mOperationalError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mOperationalError\u001b[0m: TExecuteStatementResp(status=TStatus(statusCode=3, infoMessages=['*org.apache.hive.service.cli.HiveSQLException:Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.security.AccessControlException: Permission denied: user=ceraolo, access=WRITE, inode=\"/group/five-guys/conn_table\":eric:hadoop:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:261)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1841)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1791)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7804)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\\n\\tat java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\\n):28:27', 'org.apache.hive.service.cli.operation.Operation:toSQLException:Operation.java:348', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:228', 'org.apache.hive.service.cli.operation.SQLOperation:runInternal:SQLOperation.java:265', 'org.apache.hive.service.cli.operation.Operation:run:Operation.java:260', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatementInternal:HiveSessionImpl.java:575', 'org.apache.hive.service.cli.session.HiveSessionImpl:executeStatement:HiveSessionImpl.java:550', 'sun.reflect.GeneratedMethodAccessor46:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:78', 'org.apache.hive.service.cli.session.HiveSessionProxy:access$000:HiveSessionProxy.java:36', 'org.apache.hive.service.cli.session.HiveSessionProxy$1:run:HiveSessionProxy.java:63', 'java.security.AccessController:doPrivileged:AccessController.java:-2', 'javax.security.auth.Subject:doAs:Subject.java:422', 'org.apache.hadoop.security.UserGroupInformation:doAs:UserGroupInformation.java:1730', 'org.apache.hive.service.cli.session.HiveSessionProxy:invoke:HiveSessionProxy.java:59', 'com.sun.proxy.$Proxy42:executeStatement::-1', 'org.apache.hive.service.cli.CLIService:executeStatement:CLIService.java:285', 'org.apache.hive.service.cli.thrift.ThriftCLIService:ExecuteStatement:ThriftCLIService.java:567', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1557', 'org.apache.hive.service.rpc.thrift.TCLIService$Processor$ExecuteStatement:getResult:TCLIService.java:1542', 'org.apache.thrift.ProcessFunction:process:ProcessFunction.java:39', 'org.apache.thrift.TBaseProcessor:process:TBaseProcessor.java:39', 'org.apache.hive.service.auth.TSetIpAddressProcessor:process:TSetIpAddressProcessor.java:56', 'org.apache.thrift.server.TThreadPoolServer$WorkerProcess:run:TThreadPoolServer.java:286', 'java.util.concurrent.ThreadPoolExecutor:runWorker:ThreadPoolExecutor.java:1142', 'java.util.concurrent.ThreadPoolExecutor$Worker:run:ThreadPoolExecutor.java:617', 'java.lang.Thread:run:Thread.java:745', '*org.apache.hadoop.hive.ql.metadata.HiveException:MetaException(message:java.security.AccessControlException: Permission denied: user=ceraolo, access=WRITE, inode=\"/group/five-guys/conn_table\":eric:hadoop:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:261)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1841)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1791)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7804)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\\n\\tat java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\\n):39:12', 'org.apache.hadoop.hive.ql.metadata.Hive:createTable:Hive.java:1103', 'org.apache.hadoop.hive.ql.metadata.Hive:createTable:Hive.java:1108', 'org.apache.hadoop.hive.ql.exec.DDLTask:createTable:DDLTask.java:4801', 'org.apache.hadoop.hive.ql.exec.DDLTask:execute:DDLTask.java:400', 'org.apache.hadoop.hive.ql.exec.Task:executeTask:Task.java:212', 'org.apache.hadoop.hive.ql.exec.TaskRunner:runSequential:TaskRunner.java:103', 'org.apache.hadoop.hive.ql.Driver:launchTask:Driver.java:2712', 'org.apache.hadoop.hive.ql.Driver:execute:Driver.java:2383', 'org.apache.hadoop.hive.ql.Driver:runInternal:Driver.java:2055', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1753', 'org.apache.hadoop.hive.ql.Driver:run:Driver.java:1747', 'org.apache.hadoop.hive.ql.reexec.ReExecDriver:run:ReExecDriver.java:157', 'org.apache.hive.service.cli.operation.SQLOperation:runQuery:SQLOperation.java:226', '*org.apache.hadoop.hive.metastore.api.MetaException:java.security.AccessControlException: Permission denied: user=ceraolo, access=WRITE, inode=\"/group/five-guys/conn_table\":eric:hadoop:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:261)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1841)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1791)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7804)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\\n\\tat java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\\n:59:20', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme:read:ThriftHiveMetastore.java:56296', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result$create_table_with_environment_context_resultStandardScheme:read:ThriftHiveMetastore.java:56264', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$create_table_with_environment_context_result:read:ThriftHiveMetastore.java:56190', 'org.apache.thrift.TServiceClient:receiveBase:TServiceClient.java:86', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:recv_create_table_with_environment_context:ThriftHiveMetastore.java:1592', 'org.apache.hadoop.hive.metastore.api.ThriftHiveMetastore$Client:create_table_with_environment_context:ThriftHiveMetastore.java:1578', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:create_table_with_environment_context:HiveMetaStoreClient.java:3095', 'org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient:create_table_with_environment_context:SessionHiveMetaStoreClient.java:123', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createTable:HiveMetaStoreClient.java:915', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient:createTable:HiveMetaStoreClient.java:900', 'sun.reflect.GeneratedMethodAccessor106:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.RetryingMetaStoreClient:invoke:RetryingMetaStoreClient.java:212', 'com.sun.proxy.$Proxy36:createTable::-1', 'sun.reflect.GeneratedMethodAccessor106:invoke::-1', 'sun.reflect.DelegatingMethodAccessorImpl:invoke:DelegatingMethodAccessorImpl.java:43', 'java.lang.reflect.Method:invoke:Method.java:498', 'org.apache.hadoop.hive.metastore.HiveMetaStoreClient$SynchronizedHandler:invoke:HiveMetaStoreClient.java:3001', 'com.sun.proxy.$Proxy36:createTable::-1', 'org.apache.hadoop.hive.ql.metadata.Hive:createTable:Hive.java:1092'], sqlState='08S01', errorCode=1, errorMessage='Error while processing statement: FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask. MetaException(message:java.security.AccessControlException: Permission denied: user=ceraolo, access=WRITE, inode=\"/group/five-guys/conn_table\":eric:hadoop:drwxr-xr-x\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.check(FSPermissionChecker.java:399)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:261)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSPermissionChecker.checkPermission(FSPermissionChecker.java:193)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1857)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPermission(FSDirectory.java:1841)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSDirectory.checkPathAccess(FSDirectory.java:1791)\\n\\tat org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkAccess(FSNamesystem.java:7804)\\n\\tat org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.checkAccess(NameNodeRpcServer.java:2217)\\n\\tat org.apache.hadoop.hdfs.protocolPB.ClientNamenodeProtocolServerSideTranslatorPB.checkAccess(ClientNamenodeProtocolServerSideTranslatorPB.java:1659)\\n\\tat org.apache.hadoop.hdfs.protocol.proto.ClientNamenodeProtocolProtos$ClientNamenodeProtocol$2.callBlockingMethod(ClientNamenodeProtocolProtos.java)\\n\\tat org.apache.hadoop.ipc.ProtobufRpcEngine$Server$ProtoBufRpcInvoker.call(ProtobufRpcEngine.java:524)\\n\\tat org.apache.hadoop.ipc.RPC$Server.call(RPC.java:1025)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:876)\\n\\tat org.apache.hadoop.ipc.Server$RpcCall.run(Server.java:822)\\n\\tat java.security.AccessController.doPrivileged(Native Method)\\n\\tat javax.security.auth.Subject.doAs(Subject.java:422)\\n\\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1730)\\n\\tat org.apache.hadoop.ipc.Server$Handler.run(Server.java:2682)\\n)'), operationHandle=None)"
     ]
    }
   ],
   "source": [
    "conn_table = retrieve_df_connections(\"conn_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3affea38-913b-4cb2-8716-ced7b53faf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_data = \"../data/\"\n",
    "pickle.dump(conn_table, open(path_data+\"connections_data.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa65ab2c-1877-4efb-b2a1-98c6df5070c3",
   "metadata": {},
   "source": [
    "## Trips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "3c0dd3a3-3501-41d5-b18d-d96a104d44d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_df_trips(table_name):\n",
    "    ### Create your database if it does not exist\n",
    "    \n",
    "    query = \"\"\"\n",
    "    DROP TABLE IF EXISTS {0}\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    ### Creation of the table\n",
    "    query = \"\"\"\n",
    "    CREATE EXTERNAL TABLE {0}(\n",
    "        route_id string,\n",
    "        service_id string,\n",
    "        trip_id string,\n",
    "        trip_headsign string,\n",
    "        trip_short_name string,\n",
    "        direction_id string\n",
    "    )\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ';'\n",
    "    STORED AS TEXTFILE\n",
    "    location '/group/five-guys/{0}'\n",
    "    tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM {0}\n",
    "    \"\"\".format(table_name)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [column.split('.')[1] for column in df.columns]\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    return df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f9fc76e8-ba74-400b-b55f-b891de4fc38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_trips(df):\n",
    "    routes_tot_filtered_map = df.groupby('trip_id')[['route_id', 'service_id', 'trip_headsign', 'trip_short_name', 'direction_id']].apply(lambda g: g.values.tolist()).to_dict()\n",
    "    names_cols = [\"route_id\", \"service_id\", \"trip_headsign\", \"trip_short_name\", \"direction_id\"]\n",
    "\n",
    "    map_copied = routes_tot_filtered_map.copy()\n",
    "\n",
    "    for k,v in routes_tot_filtered_map.items():\n",
    "        newdict = {}\n",
    "        for ind, element in enumerate(v[0]):\n",
    "            newdict[names_cols[ind]] = element\n",
    "        map_copied[k] = newdict\n",
    "    return map_copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c9da9542-56d9-45fb-9299-5fb01d4b32d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips_table = retrieve_df_trips(\"trips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "8b3145a6-7deb-4908-b13b-c2089cb27bb2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>route_id</th>\n",
       "      <th>service_id</th>\n",
       "      <th>trip_id</th>\n",
       "      <th>trip_headsign</th>\n",
       "      <th>trip_short_name</th>\n",
       "      <th>direction_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1-217-j19-1</td>\n",
       "      <td>TA+b0001</td>\n",
       "      <td>9.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Affoltern a. A., Bahnhof</td>\n",
       "      <td>21719</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1-217-j19-1</td>\n",
       "      <td>TA+b0001</td>\n",
       "      <td>15.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Affoltern a. A., Bahnhof</td>\n",
       "      <td>21731</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1-217-j19-1</td>\n",
       "      <td>TA+b0001</td>\n",
       "      <td>17.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Affoltern a. A., Bahnhof</td>\n",
       "      <td>21735</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1-217-j19-1</td>\n",
       "      <td>TA+b0001</td>\n",
       "      <td>19.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Affoltern a. A., Bahnhof</td>\n",
       "      <td>21739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1-217-j19-1</td>\n",
       "      <td>TA+b0001</td>\n",
       "      <td>21.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Affoltern a. A., Bahnhof</td>\n",
       "      <td>21743</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40391</th>\n",
       "      <td>80-53-Y-j19-1</td>\n",
       "      <td>TA+b0tui</td>\n",
       "      <td>50.TA.80-53-Y-j19-1.5.H</td>\n",
       "      <td>Chur</td>\n",
       "      <td>3479</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40392</th>\n",
       "      <td>80-53-Y-j19-1</td>\n",
       "      <td>TA+b0tuk</td>\n",
       "      <td>54.TA.80-53-Y-j19-1.16.H</td>\n",
       "      <td>Chur</td>\n",
       "      <td>3481</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40393</th>\n",
       "      <td>80-55-Y-j19-1</td>\n",
       "      <td>TA+b0tvm</td>\n",
       "      <td>25.TA.80-55-Y-j19-1.12.H</td>\n",
       "      <td>Zürich HB</td>\n",
       "      <td>3464</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40394</th>\n",
       "      <td>80-55-Y-j19-1</td>\n",
       "      <td>TA+b0tvn</td>\n",
       "      <td>33.TA.80-55-Y-j19-1.12.H</td>\n",
       "      <td>Zürich HB</td>\n",
       "      <td>3468</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40395</th>\n",
       "      <td>80-55-Y-j19-1</td>\n",
       "      <td>TA+b0tvo</td>\n",
       "      <td>43.TA.80-55-Y-j19-1.12.H</td>\n",
       "      <td>Zürich HB</td>\n",
       "      <td>3472</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>40396 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            route_id service_id                   trip_id  \\\n",
       "0        1-217-j19-1   TA+b0001      9.TA.1-217-j19-1.1.H   \n",
       "1        1-217-j19-1   TA+b0001     15.TA.1-217-j19-1.1.H   \n",
       "2        1-217-j19-1   TA+b0001     17.TA.1-217-j19-1.1.H   \n",
       "3        1-217-j19-1   TA+b0001     19.TA.1-217-j19-1.1.H   \n",
       "4        1-217-j19-1   TA+b0001     21.TA.1-217-j19-1.1.H   \n",
       "...              ...        ...                       ...   \n",
       "40391  80-53-Y-j19-1   TA+b0tui   50.TA.80-53-Y-j19-1.5.H   \n",
       "40392  80-53-Y-j19-1   TA+b0tuk  54.TA.80-53-Y-j19-1.16.H   \n",
       "40393  80-55-Y-j19-1   TA+b0tvm  25.TA.80-55-Y-j19-1.12.H   \n",
       "40394  80-55-Y-j19-1   TA+b0tvn  33.TA.80-55-Y-j19-1.12.H   \n",
       "40395  80-55-Y-j19-1   TA+b0tvo  43.TA.80-55-Y-j19-1.12.H   \n",
       "\n",
       "                  trip_headsign trip_short_name direction_id  \n",
       "0      Affoltern a. A., Bahnhof           21719            0  \n",
       "1      Affoltern a. A., Bahnhof           21731            0  \n",
       "2      Affoltern a. A., Bahnhof           21735            0  \n",
       "3      Affoltern a. A., Bahnhof           21739            0  \n",
       "4      Affoltern a. A., Bahnhof           21743            0  \n",
       "...                         ...             ...          ...  \n",
       "40391                      Chur            3479            0  \n",
       "40392                      Chur            3481            0  \n",
       "40393                 Zürich HB            3464            0  \n",
       "40394                 Zürich HB            3468            0  \n",
       "40395                 Zürich HB            3472            0  \n",
       "\n",
       "[40396 rows x 6 columns]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trips_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "6a2f8da1-b92e-4a74-93f5-3a029cd73c7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "trips = process_trips(trips_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "6d0ec662-03bb-4d9f-9441-e5d1a9776dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_data = \"../data/\"\n",
    "pickle.dump(trips, open(path_data+\"trips.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7b501d9-ba05-49d2-89f0-7094bd74d6f2",
   "metadata": {},
   "source": [
    "## Stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "0d25e811-e236-4df6-a86b-6710f65e5da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "query = \"\"\"\n",
    "    DROP TABLE IF EXISTS {0}.csv\n",
    "    \"\"\".format(\"stops_table\")\n",
    "cur.execute(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "54842bad-e7f4-4a36-87b2-031c5227237c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_df_stops(table_name):\n",
    "    ### Create your database if it does not exist\n",
    "    \n",
    "    query = \"\"\"\n",
    "    DROP TABLE IF EXISTS {0}.csv\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "#     ### Creation of the table\n",
    "#     query = \"\"\"\n",
    "#     CREATE EXTERNAL TABLE stopss(\n",
    "#         stop_id string,\n",
    "#         stop_name string,\n",
    "#         stop_lat string,\n",
    "#         stop_lon string,\n",
    "#         parent_station string\n",
    "#     )\n",
    "#     ROW FORMAT DELIMITED\n",
    "#     FIELDS TERMINATED BY ';'\n",
    "#     STORED AS TEXTFILE\n",
    "#     location '/group/five-guys/{0}'\n",
    "#     tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "#     \"\"\".format(table_name)\n",
    "#     cur.execute(query)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM stopss\n",
    "    \"\"\".format(table_name)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [column.split('.')[1] for column in df.columns]\n",
    "    \n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "e64ebe91-4085-4137-85e8-0456ddc643d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = retrieve_df_stops(\"stops_table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "dfe6672a-e1a7-49dd-8920-685ede9d2bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "close_stops_map = stops.groupby('stop_id')[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'parent_station']].apply(lambda g: g.values.tolist()).to_dict()\n",
    "names_cols = ['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'parent_station']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "66045e60-2091-4152-9974-41a4e98bb3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_stops(df):\n",
    "    close_stops_map = df.groupby('stop_id')[['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'parent_station']].apply(lambda g: g.values.tolist()).to_dict()\n",
    "    names_cols = ['stop_id', 'stop_name', 'stop_lat', 'stop_lon', 'parent_station']\n",
    "    \n",
    "    stops_final = close_stops_map.copy()\n",
    "\n",
    "    for k,v in close_stops_map.items():\n",
    "        newdict = {}\n",
    "        for ind, element in enumerate(v[0]):\n",
    "            newdict[names_cols[ind]] = element\n",
    "            if names_cols[ind] == \"parent_station\" and isinstance(element, str) and element!=\"\":\n",
    "                if element[:6]==\"Parent\":\n",
    "                    element = element[6:]\n",
    "                    newdict[names_cols[ind]] = element\n",
    "                if element[-1]==\"P\":\n",
    "                    element = element[:-1]\n",
    "                    newdict[names_cols[ind]] = element\n",
    "                if element != str(int(element)):\n",
    "                    print(element)\n",
    "        stops_final[k] = newdict\n",
    "    \n",
    "    return stops_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "de87935b-75e6-46c4-a676-e2686fe6caf5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "stops_dict = process_stops(stops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "70100051-422a-4d59-ad07-cbf7390604ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_data = \"../data/\"\n",
    "pickle.dump(stops_dict, open(path_data+\"stops.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e1df373-bf08-43d3-9a8a-4965d5ccdb55",
   "metadata": {},
   "source": [
    "## Footpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2dd19ade-b17e-4180-906e-531699b8c2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_df_footpath(table_name):\n",
    "    ### Create your database if it does not exist\n",
    "    \n",
    "    query = \"\"\"\n",
    "    DROP TABLE IF EXISTS {0}\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    ### Creation of the table\n",
    "    query = \"\"\"\n",
    "    CREATE EXTERNAL TABLE {0}(\n",
    "        departure_id string,\n",
    "        arrival_id string,\n",
    "        `time` int\n",
    "    )\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ';'\n",
    "    STORED AS TEXTFILE\n",
    "    location '/group/five-guys/{0}'\n",
    "    tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM {0}\n",
    "    \"\"\".format(table_name)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [column.split('.')[1] for column in df.columns]\n",
    "    \n",
    "    footpath_map = df.groupby('arrival_id')[['departure_id', 'time']].apply(lambda g: g.values.tolist()).to_dict()\n",
    "    footpath_map = {k: dict(v) for k, v in footpath_map.items()}\n",
    "    \n",
    "    \n",
    "    del df\n",
    "    \n",
    "    return footpath_map\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78038ac3-d1d4-4dae-997b-3ab053eaa842",
   "metadata": {},
   "outputs": [],
   "source": [
    "footpath = retrieve_df_footpath(\"footpath\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f388fc-f230-4dcf-aaef-697df2c5f079",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(footpath, open(path_data+\"footpath.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4686ea0a-fb9f-4317-bb34-ad33fdd10947",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#TO GET OBJECTS FROM PICKLE\n",
    "file = open(path_data+\"trips.pickle\",'rb')\n",
    "object_file = pickle.load(file)\n",
    "file.close()\n",
    "#object_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6434798c-2380-4fd3-b484-4a839ef4422d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_df_routes(table_name):\n",
    "    ### Create your database if it does not exist\n",
    "    \n",
    "    query = \"\"\"\n",
    "    DROP TABLE IF EXISTS {0}\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    ### Creation of the table\n",
    "    query = \"\"\"\n",
    "    CREATE EXTERNAL TABLE {0}(\n",
    "        trip_id string,\n",
    "        routes_desc string\n",
    "    )\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ';'\n",
    "    STORED AS TEXTFILE\n",
    "    location '/group/five-guys/{0}'\n",
    "    tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT *\n",
    "    FROM {0}\n",
    "    \"\"\".format(table_name)\n",
    "    df = pd.read_sql(query, conn)\n",
    "    df.columns = [column.split('.')[1] for column in df.columns]\n",
    "    \n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8a79477-6825-4838-86e1-b0a453702c6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>trip_id</th>\n",
       "      <th>routes_desc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>9.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>17.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>19.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.TA.1-217-j19-1.1.H</td>\n",
       "      <td>Bus</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 trip_id routes_desc\n",
       "0   9.TA.1-217-j19-1.1.H         Bus\n",
       "1  15.TA.1-217-j19-1.1.H         Bus\n",
       "2  17.TA.1-217-j19-1.1.H         Bus\n",
       "3  19.TA.1-217-j19-1.1.H         Bus\n",
       "4  21.TA.1-217-j19-1.1.H         Bus"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "routes = retrieve_df_routes(\"routes\")\n",
    "routes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb2d1992-0e75-440e-8ed0-e742d2580070",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "path_data = \"../data/\"\n",
    "pickle.dump(routes, open(path_data+\"routes.pickle\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bba70dd3-1ab9-4028-9c98-12a86adc9dff",
   "metadata": {},
   "source": [
    "## Confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e9e5a561-5630-48a8-8c5a-b6af49554d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_df_confidence(table_name):\n",
    "    ### Create your database if it does not exist\n",
    "    \n",
    "    query = \"\"\"\n",
    "    DROP TABLE IF EXISTS {0}\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    ### Creation of the table\n",
    "    query = \"\"\"\n",
    "    CREATE EXTERNAL TABLE {0}(\n",
    "       TRIP_ID string,\n",
    "       DEPARTURE_ID string,\n",
    "       ARRIVAL_ID string,\n",
    "       DAY_OF_WEEK int,\n",
    "       MAX_ARRIVAL_DELAY float,\n",
    "       CUMULATIVE float\n",
    "    )\n",
    "    ROW FORMAT DELIMITED\n",
    "    FIELDS TERMINATED BY ';'\n",
    "    STORED AS TEXTFILE\n",
    "    location '/group/five-guys/{0}'\n",
    "    tblproperties (\"skip.header.line.count\"=\"1\")\n",
    "    \"\"\".format(table_name)\n",
    "    cur.execute(query)\n",
    "    \n",
    "    query = \"\"\"\n",
    "    SELECT DEPARTURE_ID,\n",
    "           ARRIVAL_ID,\n",
    "           DAY_OF_WEEK,\n",
    "           MAX_ARRIVAL_DELAY,\n",
    "           CUMULATIVE\n",
    "    FROM {0}\n",
    "    \"\"\".format(table_name)\n",
    "    df = pd.read_sql(query, conn)\n",
    "#    df.columns = [column.split('.')[1] for column in df.columns]\n",
    "\n",
    "    #confidence = list(df.itertuples(index=False, name=None))\n",
    "    \n",
    "#     del df\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "915e4ee6-ec45-430a-b1fd-0aa339da848f",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence = retrieve_df_confidence(\"confidence_monday\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528fdfaf-3083-4dc3-9d74-65df4d7a5df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c31b5b-5a93-4fc4-b543-1dd79a30d69b",
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(confidence, open(path_data+\"confidence.pickle\", \"wb\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
